{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81eebe80",
   "metadata": {},
   "source": [
    "# Clustering for dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ddbd1",
   "metadata": {},
   "source": [
    "### Clustering 2D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(points)\n",
    "\n",
    "# Determine the cluster labels of new_points: labels\n",
    "labels = model.predict(new_points)\n",
    "\n",
    "# Print cluster labels of new_points\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a45a6d",
   "metadata": {},
   "source": [
    "### Inspect your clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ab915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the columns of new_points: xs and ys\n",
    "xs = new_points[:,0]\n",
    "ys = new_points[:,1]\n",
    "\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs,ys, c=labels, alpha=0.5)\n",
    "\n",
    "# Assign the cluster centers: centroids\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "plt.scatter(centroids_x, centroids_y, marker=\"D\", s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36234a8",
   "metadata": {},
   "source": [
    "### How many clusters of grain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(samples)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff3836",
   "metadata": {},
   "source": [
    "### Evaluating the grain clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fee1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KMeans model with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Use fit_predict to fit model and obtain cluster labels: labels\n",
    "labels = model.fit_predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'],df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7ecf7",
   "metadata": {},
   "source": [
    "##### pd.crosstab\n",
    "범주형 변수로 되어있는 요인(factors)별로 교차분석(cross tabulations) 해서, 행, 열 요인 기준 별로 빈도를 세어서 도수분포표(frequency table), 교차표(contingency table) 를 만들어주는 역할을 함     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a7cca",
   "metadata": {},
   "source": [
    "### Scaling fish data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ace1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler,kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b57e7",
   "metadata": {},
   "source": [
    "##### make_pipeline\n",
    "데이터 전처리에서 학습까지의 과정을 하나로 연결해주는 역할    \n",
    "코드의 길이가 파이프라인을 사용하면 확연히 줄어든다   \n",
    "\n",
    "사용하는 방법 : 변수 선택 -> 표준화 -> 모형학습    \n",
    "\n",
    "장점  \n",
    "- train과 test 데이터 손실을 피할 수 있음 \n",
    "- 교차 검증 및 기타 모델 선택 유형을 쉽게 함 \n",
    "- 재현성 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca573f",
   "metadata": {},
   "source": [
    "### Clustering the fish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the pipeline to samples\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and species as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'species': species})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'],df['species'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa7eaf",
   "metadata": {},
   "source": [
    "### Clustering stocks using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Normalizer\n",
    "from sklearn.preporcessing import normalize\n",
    "\n",
    "# Create a normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a KMeans model with 10 clusters: kmeans\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "pipeline = make_pipeline(normalizer,kmeans)\n",
    "\n",
    "# Fit pipeline to the daily price movements\n",
    "pipeline.fit(movements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a446e",
   "metadata": {},
   "source": [
    "### Which stocks move together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f81e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Predict the cluster labels: labels\n",
    "labels = pipeline.predict(movements)\n",
    "\n",
    "# Create a DataFrame aligning labels and companies: df\n",
    "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a28e09",
   "metadata": {},
   "source": [
    "\n",
    "         \n",
    "# Visualization with hierarchical clustering and t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ddf52",
   "metadata": {},
   "source": [
    "### Hierarchical clustering of the grain data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the linkage: mergings - 계층적인 구조로 확인할 수 있음 \n",
    "mergings = linkage(samples, method=\"complete\")\n",
    "\n",
    "# Plot the dendrogram, using varieties as labels\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392bc9d",
   "metadata": {},
   "source": [
    "##### 덴드로그램\n",
    "\n",
    "클러스터링의 결과를 시각화하기 위한 대표적인 그래프      \n",
    "대표적으로 계층적 군집분석 방식에 대해 시각화하는 그래프      \n",
    "즉 가까운 두 점 혹은 그룹을 이뤄나가는 과정을 시각화하는 것   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943a1cc",
   "metadata": {},
   "source": [
    "### Hierarchies of stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c74244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the movements: normalized_movements\n",
    "normalized_movements = normalize(movements)\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = linkage(normalized_movements, method=\"complete\")\n",
    "\n",
    "# Plot the dendrogram\n",
    "dendrogram(mergings,\n",
    "           labels=companies,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7cfb4a",
   "metadata": {},
   "source": [
    "### Different linkage, different hierarchical clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6462b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Calculate the linkage: mergings\n",
    "mergings = mergings = linkage(samples, method=\"single\")\n",
    "\n",
    "# Plot the dendrogram\n",
    "dendrogram(mergings,\n",
    "           labels=country_names,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ea289",
   "metadata": {},
   "source": [
    "### Extracting the cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff375c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Use fcluster to extract labels: labels\n",
    "labels = fcluster(mergings, 6, criterion='distance')\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'],df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedcc30",
   "metadata": {},
   "source": [
    "### t-SNE visualization of grain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ef5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=200)\n",
    "\n",
    "# Apply fit_transform to samples: tsne_features\n",
    "tsne_features = model.fit_transform(samples)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1st feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot, coloring by variety_numbers\n",
    "plt.scatter(xs, ys, c=variety_numbers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab458fb0",
   "metadata": {},
   "source": [
    "##### TNSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318feaf",
   "metadata": {},
   "source": [
    "### A t-SNE map of the stock market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c91fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=50)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = model.fit_transform(normalized_movements)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:,0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5)\n",
    "\n",
    "# Annotate the points\n",
    "for x, y, company in zip(xs, ys, companies):\n",
    "    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a3abd",
   "metadata": {},
   "source": [
    "# Decorrelating your data and dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa20af",
   "metadata": {},
   "source": [
    "### Correlated data in nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assign the 0th column of grains: width\n",
    "width = grains[:,0]\n",
    "\n",
    "# Assign the 1st column of grains: length\n",
    "length = grains[:,1]\n",
    "\n",
    "# Scatter plot width vs length\n",
    "plt.scatter(width, length)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation\n",
    "correlation, pvalue = pearsonr(width, length)\n",
    "\n",
    "# Display the correlation\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448f708",
   "metadata": {},
   "source": [
    "### Decorrelating the grain measurements with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Apply the fit_transform method of model to grains: pca_features\n",
    "pca_features = model.fit_transform(grains)\n",
    "\n",
    "# Assign 0th column of pca_features: xs\n",
    "xs = pca_features[:,0]\n",
    "\n",
    "# Assign 1st column of pca_features: ys\n",
    "ys = pca_features[:,1]\n",
    "\n",
    "# Scatter plot xs vs ys\n",
    "plt.scatter(xs, ys)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation of xs and ys\n",
    "correlation, pvalue = pearsonr(xs, ys)\n",
    "\n",
    "# Display the correlation\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5fa19",
   "metadata": {},
   "source": [
    "### The first principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of the untransformed points\n",
    "plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(grains)\n",
    "\n",
    "# Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]\n",
    "\n",
    "# Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9fc21",
   "metadata": {},
   "source": [
    "### Variance of the PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bca0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to 'samples'\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f8cbc",
   "metadata": {},
   "source": [
    "### Dimension reduction of the fish measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df17c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA model with 2 components: pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA instance to the scaled samples\n",
    "pca.fit(scaled_samples)\n",
    "\n",
    "# Transform the scaled samples: pca_features\n",
    "pca_features = pca.transform(scaled_samples)\n",
    "\n",
    "# Print the shape of pca_features\n",
    "print(pca_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2bf5d",
   "metadata": {},
   "source": [
    "### A tf-idf word-frequency array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = TfidfVectorizer() \n",
    "\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "# Print result of toarray() method\n",
    "print(csr_mat.toarray())\n",
    "\n",
    "# Get the words: words\n",
    "words = tfidf.get_feature_names()\n",
    "\n",
    "# Print words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bfe17",
   "metadata": {},
   "source": [
    "### Clustering Wikipedia part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d924daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a TruncatedSVD instance: svd\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "# Create a KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(svd, kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ebf52",
   "metadata": {},
   "source": [
    "### Clustering Wikipedia part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the pipeline to articles\n",
    "pipeline.fit(articles)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(articles)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles: df\n",
    "df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0f143",
   "metadata": {},
   "source": [
    "# Discovering interpretable features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed2c19",
   "metadata": {},
   "source": [
    "### NMF applied to Wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance: model\n",
    "model = NMF(n_components=6)\n",
    "\n",
    "# Fit the model to articles\n",
    "model.fit(articles)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(articles)\n",
    "\n",
    "# Print the NMF features\n",
    "print(nmf_features.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91550402",
   "metadata": {},
   "source": [
    "### NMF features of the Wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54679e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a pandas DataFrame: df\n",
    "df = pd.DataFrame(nmf_features, index=titles)\n",
    "\n",
    "# Print the row for 'Anne Hathaway'\n",
    "print(df.loc['Anne Hathaway'])\n",
    "\n",
    "# Print the row for 'Denzel Washington'\n",
    "print(df.loc['Denzel Washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e66f02",
   "metadata": {},
   "source": [
    "### NMF learns topics of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab81eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: components_df\n",
    "components_df = pd.DataFrame(model.components_, columns=words)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(components_df.shape)\n",
    "\n",
    "# Select row 3: component\n",
    "component = components_df.iloc[3]\n",
    "\n",
    "# Print result of nlargest\n",
    "print(component.nlargest())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919bdaa6",
   "metadata": {},
   "source": [
    "### Explore the LED digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ee7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Select the 0th row: digit\n",
    "digit = samples[0,:]\n",
    "\n",
    "# Print digit\n",
    "print(digit)\n",
    "\n",
    "# Reshape digit to a 13x8 array: bitmap\n",
    "bitmap = digit.reshape((13, 8))\n",
    "\n",
    "# Print bitmap\n",
    "print(bitmap)\n",
    "\n",
    "# Use plt.imshow to display bitmap\n",
    "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab93ac",
   "metadata": {},
   "source": [
    "### NMF learns the parts of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2702296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF model: model\n",
    "model = NMF(n_components=7)\n",
    "\n",
    "# Apply fit_transform to samples: features\n",
    "features = model.fit_transform(samples)\n",
    "\n",
    "# Call show_as_image on each component\n",
    "for component in model.components_:\n",
    "    show_as_image(component)\n",
    "\n",
    "# Select the 0th row of features: digit_features\n",
    "digit_features = features[0,:]\n",
    "\n",
    "# Print digit_features\n",
    "print(digit_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008fba58",
   "metadata": {},
   "source": [
    "### PCA doesn't learn parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ccea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA(n_components=7)\n",
    "\n",
    "# Apply fit_transform to samples: features\n",
    "features = model.fit_transform(samples)\n",
    "\n",
    "# Call show_as_image on each component\n",
    "for component in model.components_:\n",
    "    show_as_image(component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d440b",
   "metadata": {},
   "source": [
    "### Which articles are similar to 'Cristiano Ronaldo'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the NMF features: norm_features\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=titles)\n",
    "\n",
    "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
    "article = df.loc['Cristiano Ronaldo']\n",
    "\n",
    "# Compute the dot products: similarities\n",
    "similarities = df.dot(article)\n",
    "\n",
    "# Display those with the largest cosine similarity\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fea18",
   "metadata": {},
   "source": [
    "### Recommend musical artists part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58ef72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a MaxAbsScaler: scaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Create an NMF model: nmf\n",
    "nmf = NMF(n_components=20)\n",
    "\n",
    "# Create a Normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
    "\n",
    "# Apply fit_transform to artists: norm_features\n",
    "norm_features = pipeline.fit_transform(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff26628",
   "metadata": {},
   "source": [
    "### Recommend musical artists part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74cc7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=artist_names)\n",
    "\n",
    "# Select row of 'Bruce Springsteen': artist\n",
    "artist = df.loc['Bruce Springsteen']\n",
    "\n",
    "# Compute cosine similarities: similarities\n",
    "similarities = df.dot(artist)\n",
    "\n",
    "# Display those with highest cosine similarity\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee2250",
   "metadata": {},
   "source": [
    "## 코드 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6d1db",
   "metadata": {},
   "source": [
    "### 1) pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211fcae",
   "metadata": {},
   "source": [
    "데이터 전처리에서 학습까지의 과정을 하나로 연결해주는 역할    \n",
    "코드의 길이가 파이프라인을 사용하면 확연히 줄어든다   \n",
    "\n",
    "사용하는 방법 : 변수 선택 -> 표준화 -> 모형학습    \n",
    "\n",
    "장점  \n",
    "- train과 test 데이터 손실을 피할 수 있음 \n",
    "- 교차 검증 및 기타 모델 선택 유형을 쉽게 함 \n",
    "- 재현성 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654b8740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=['sepal_length','sepal_width','petal_length','petal_width']) # x\n",
    "target = iris.target # y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5dd34f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이프라인 생성\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC # 서포트 벡터\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=11)\n",
    "\n",
    "pipe = Pipeline([('scaler',StandardScaler()),('svc',SVC())]) # 표준화, 학습\n",
    "pipe.fit(X_train, y_train) # 모형학습\n",
    "pipe.score(X_test, y_test) # 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ce819a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC())])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ss = StandardScaler()\n",
    "svc = SVC()\n",
    "\n",
    "pipeline = make_pipeline(ss, svc) # 작업 클래스만 차례대로 넣어주면 작업명을 자동으로 생성\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bffb50",
   "metadata": {},
   "source": [
    "### 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c863801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
