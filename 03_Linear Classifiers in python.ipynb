{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896bdad5",
   "metadata": {},
   "source": [
    "# 01) Applying logistic regression and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c82c05",
   "metadata": {},
   "source": [
    "### KNN classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and fit the model\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test features, print the results\n",
    "pred = knn.predict(X_test)[0]\n",
    "print(\"Prediction for test example 0:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb3f16",
   "metadata": {},
   "source": [
    "- 군집화의 크기가 클 수록 높은 테스트 정확도를 나타낸다\n",
    "\n",
    "- 과적합된 경우 : \n",
    "    Training accuracy 95%, testing accuracy 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09cac40",
   "metadata": {},
   "source": [
    "### Running LogisticRegression and SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b2955",
   "metadata": {},
   "source": [
    "###### 로지스틱 회귀분석과 서포트 벡터 머신\n",
    "\n",
    "###### SVM(서포트 벡터 머신) : 데이터셋을 분류하는 할 때의 비선형방법 중 하나  \n",
    "knn 과 같이 기존 데이터를 사용해서 새로운 데이터가 입력되었을 때 분류하는 방법   \n",
    " https://losskatsu.github.io/machine-learning/svm/#%EB%84%88%EB%B9%84width-%EC%B5%9C%EB%8C%80%ED%99%94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614f8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4cd2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajm10\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9511111111111111\n",
      "0.9970304380103935\n",
      "0.9844444444444445\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "# Apply SVM and print scores\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_train, y_train))\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c035671",
   "metadata": {},
   "source": [
    "### Sentiment analysis for movie reviews\n",
    "영화 리뷰들에 대한 감정분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression and train\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Predict sentiment for a glowing review\n",
    "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
    "review1_features = get_features(review1)\n",
    "print(\"Review:\", review1)\n",
    "print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# Predict sentiment for a poor review\n",
    "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
    "review2_features = get_features(review2)\n",
    "print(\"Review:\", review2)\n",
    "print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b93192",
   "metadata": {},
   "source": [
    "### Visualizing decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X,y)\n",
    "\n",
    "# Plot the classifiers\n",
    "plot_4_classifiers(X, y, classifiers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2682c",
   "metadata": {},
   "source": [
    "# 02) Loss functions\n",
    "머신러닝에서 학습은 (그중에서도 supervised learning은) 학습데이터를 기반으로 실제값과 예측값 사이의 Loss(오차)를 최소화하도록 weight(가중치)와 bias(편향)을 최적화하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714e4f1",
   "metadata": {},
   "source": [
    "### Changing the model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the coefficients\n",
    "model.coef_ = np.array([[0,1]])\n",
    "model.intercept_ = np.array([0])\n",
    "\n",
    "# Plot the data and decision boundary\n",
    "plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "num_err = np.sum(y != model.predict(X))\n",
    "print(\"Number of errors:\", num_err)\n",
    "\n",
    "####################################\n",
    "\n",
    "# Set the coefficients\n",
    "model.coef_ = np.array([[-1,1]]) #  이 부분이 변경됐음 - 회귀계수\n",
    "model.intercept_ = np.array([-3]) # y 절편\n",
    "\n",
    "# Plot the data and decision boundary\n",
    "plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "num_err = np.sum(y != model.predict(X))\n",
    "print(\"Number of errors:\", num_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147ab86",
   "metadata": {},
   "source": [
    "### Minimizing a loss function\n",
    "##### scipy.optimize.minimize : 함수 최적화 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_true-y_i_pred)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print( lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a0b71",
   "metadata": {},
   "source": [
    "### Comparing the logistic and hinge losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f34939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical functions for logistic and hinge losses\n",
    "def log_loss(raw_model_output):\n",
    "    return np.log(1+np.exp(-raw_model_output))\n",
    "def hinge_loss(raw_model_output):\n",
    "    return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-2,2,1000)\n",
    "plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db672c8e",
   "metadata": {},
   "source": [
    "# 03) Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e810c",
   "metadata": {},
   "source": [
    "### Implementing logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef197ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logistic loss, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        raw_model_output = w@X[i]\n",
    "        s = s + log_loss(raw_model_output * y[i])\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LogisticRegression\n",
    "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da4c74",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validaton errors initialized as empty list\n",
    "train_errs = list()\n",
    "valid_errs = list()\n",
    "\n",
    "# Loop over values of C_value\n",
    "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    # Create LogisticRegression object and fit\n",
    "    lr = LogisticRegression(C=C_value)\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate error rates and append to lists\n",
    "    train_errs.append( 1.0 - lr.score(X_train, y_train) )\n",
    "    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n",
    "    \n",
    "# Plot results\n",
    "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "plt.legend((\"train\", \"validation\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df9ddc",
   "metadata": {},
   "source": [
    "### Logistic regression and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2264d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify L1 regularization\n",
    "lr = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "## Instantiate a logistic regression object that uses L1 regularization.\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "best_lr = searcher.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print(\"Total number of features:\", coefs.size)\n",
    "print(\"Number of selected features:\", np.count_nonzero(coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638ffb0",
   "metadata": {},
   "source": [
    "### Identifying the most positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the sorted cofficients\n",
    "inds_ascending = np.argsort(lr.coef_.flatten()) \n",
    "inds_descending = inds_ascending[::-1]\n",
    "\n",
    "# Print the most positive words\n",
    "print(\"Most positive words: \", end=\"\")\n",
    "for i in range(5):\n",
    "    print(vocab[inds_descending[i]], end=\", \")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print most negative words\n",
    "print(\"Most negative words: \", end=\"\")\n",
    "for i in range(5):\n",
    "    print(vocab[inds_ascending[i]], end=\", \")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737035a",
   "metadata": {},
   "source": [
    "### Regularization and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf590e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the regularization strength\n",
    "model = LogisticRegression(C=0.1)\n",
    "\n",
    "# Fit and plot\n",
    "model.fit(X,y)\n",
    "plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob = model.predict_proba(X)\n",
    "print(\"Maximum predicted probability\", np.max(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50634c",
   "metadata": {},
   "source": [
    "### Visualizing easy and difficult examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b698c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "# Get predicted probabilities\n",
    "proba = lr.predict_proba(X)\n",
    "\n",
    "# Sort the example indices by their maximum probability\n",
    "proba_inds = np.argsort(np.max(proba,axis=1))\n",
    "\n",
    "# Show the most confident (least ambiguous) digit\n",
    "show_digit(proba_inds[-1], lr)\n",
    "\n",
    "# Show the least confident (most ambiguous) digit\n",
    "show_digit(proba_inds[0], lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290623f6",
   "metadata": {},
   "source": [
    "### Fitting multi-class logistic regression\n",
    "\n",
    "##### ovo(OneVsOne 일대 일)\n",
    "- 클래스 쌍당 하나의 분류기 생성\n",
    "\n",
    "##### ovr(One-vs-rest 일대 나머지)  \n",
    "- OneVsRestClassifier는 모든 class별로 모델을 피팅\n",
    "- 각 분류기에 대해 클래스는 모든 클래스에 대해 피팅\n",
    "\n",
    "##### Multilabel classification\n",
    "- 다중 레이블 분류는 가능한 클래스의 m 레이블로 각 샘플에 레이블을 지정하는 분류 작업\n",
    "- 공식적으로 모든 샘플에 대해 이진 출력이 각 클래스에 할당\n",
    "- positive = 0 , negative = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edeb1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit one-vs-rest logistic regression classifier\n",
    "lr_ovr = LogisticRegression(multi_class=\"ovr\")\n",
    "lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# Fit softmax classifier\n",
    "lr_mn = LogisticRegression(multi_class=\"multinomial\")\n",
    "lr_mn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8eb6b6",
   "metadata": {},
   "source": [
    "### Visualizing multi-class logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eae72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training accuracies\n",
    "print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "\n",
    "# Create the binary classifier (class 1 vs. rest)\n",
    "lr_class_1 = LogisticRegression(C=100)\n",
    "lr_class_1.fit(X_train, y_train==1)\n",
    "\n",
    "# Plot the binary classifier (class 1 vs. rest)\n",
    "plot_classifier(X_train, y_train==1, lr_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d1c09",
   "metadata": {},
   "source": [
    "### One vs rest SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use SVC instead of LinearSVC from now on\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create/plot the binary classifier (class 1 vs. rest)\n",
    "svm_class_1 = SVC()\n",
    "svm_class_1.fit(X_train, y_train==1)\n",
    "plot_classifier(X_train, y_train==1, svm_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96d6dc",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b080d",
   "metadata": {},
   "source": [
    "### Effect of removing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ef7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X,y)\n",
    "plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
    "\n",
    "# Make a new data set keeping only the support vectors\n",
    "print(\"Number of original examples\", len(X))\n",
    "print(\"Number of support vectors\", len(svm.support_))\n",
    "X_small = X[svm.support_]\n",
    "y_small = y[svm.support_]\n",
    "\n",
    "# Train a new SVM using only the support vectors\n",
    "svm_small = SVC(kernel=\"linear\")\n",
    "svm_small.fit(X_small, y_small)\n",
    "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c227ec2",
   "metadata": {},
   "source": [
    "### GridSearchCV warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X, y)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e4fbd",
   "metadata": {},
   "source": [
    "### Jointly tuning gamma and C with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704fb46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X_train,y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "\n",
    "# Report the test accuracy using these best parameters\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f8012",
   "metadata": {},
   "source": [
    "### Using SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set random_state=0 for reproducibility \n",
    "linear_classifier = SGDClassifier(random_state=0)\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
    "             'loss':['hinge', 'log_loss']}\n",
    "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c143a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
