{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac795206",
   "metadata": {},
   "source": [
    "# 01) Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67bc5c",
   "metadata": {},
   "source": [
    "### Train your first classification tree\n",
    "\n",
    "결정트리분류기 = DecisionTreeClassifier\n",
    "\n",
    "- 분할과 가지치기 과정을 반복하면서 모델을 생성한다.\n",
    "- 결정트리에는 분류와 회귀 모두에 사용할 수 있다.\n",
    "- 여러개의 모델을 함께 사용하는 앙상블 모델이 존재한다. (RandomForest, GradientBoosting, XGBoost)\n",
    "- 각 특성이 개별 처리되기 때문에 데이터 스케일에 영향을 받지 않아 특성의 정규화나 표준화가 필요 없다.\n",
    "- 시계열 데이터와 같이 범위 밖의 포인트는 예측 할 수 없다.\n",
    "- 과대적합되는 경향이 있다. 이는 본문에 소개할 가지치기 기법을 사용해도 크게 개선되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=SEED)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4cdbc",
   "metadata": {},
   "source": [
    "### Evaluate the classification tree\n",
    "\n",
    "accuracy_score : 실제 라벨과 예측 라벨이 일치하는 비율을 나타낸 지표\n",
    "- 실제 라벨과 예측 라벨 결과를 차례대로 input으로 넣으면 정확도를 계산하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6756c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f34d7",
   "metadata": {},
   "source": [
    "### Logistic regression vs classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "\n",
    "# Instatiate logreg\n",
    "logreg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Fit logreg to the training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Define a list called clfs containing the two classifiers logreg and dt\n",
    "clfs = [logreg, dt]\n",
    "\n",
    "# Review the decision regions of the two classifiers\n",
    "plot_labeled_decision_regions(X_test, y_test, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bef7a1",
   "metadata": {},
   "source": [
    "### Using entropy as a criterion\n",
    "\n",
    "- 불순한 상태는 분류하기 어려운 상태로 볼 수 있으며 불순한 상태일 수록 엔트로피의 수치는 커지며 불순도가 높을수록 분류하기 어렵다\n",
    "- 불순도와 엔트로피는 비례관계\n",
    "의사 결정 나무 모델은 각 노드의 순도가 증가하거나 불순도 또는 불확실성이 감소하는 방향으로 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0bb7f5",
   "metadata": {},
   "source": [
    "### Entropy vs Gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60633e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy_score from sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred= dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print(f'Accuracy achieved by using entropy: {accuracy_entropy:.3f}')\n",
    "\n",
    "# Print accuracy_gini\n",
    "print(f'Accuracy achieved by using the gini index: {accuracy_gini:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81a37c",
   "metadata": {},
   "source": [
    "### Train your first regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5da12",
   "metadata": {},
   "source": [
    "### Evaluate the regression tree\n",
    "\n",
    "MSE\n",
    "- 회귀방정식 평가지표 중 하나로 사용\n",
    "- MAE와는 다르게 모델의 예측값과 실제값 차이의 면적의 합입니다. 이런 차이로, 특이값이 존재하면 수치가 많이 늘어난다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e73217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt ** 0.5\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b30add",
   "metadata": {},
   "source": [
    "### Linear regression vs regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2595d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set labels \n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)\n",
    "\n",
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr ** 0.5\n",
    "\n",
    "# Print rmse_lr\n",
    "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n",
    "\n",
    "# Print rmse_dt\n",
    "print('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1786d8f",
   "metadata": {},
   "source": [
    "# 02) The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfa673",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4503c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=4,\n",
    "             min_samples_leaf=0.26, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574446d",
   "metadata": {},
   "source": [
    "### Evaluate the 10-fold CV error\n",
    "\n",
    "\n",
    "##### cross_val_score\n",
    "- 교차검증 모델의 종류 중 하나 \n",
    "- 폴드세트 추출, 학습/예측, 평가를 한번에 수행합니다. 교차검증을 보다 편리하게 수행 가능\n",
    "- scoring = 'neg_mean_squared_error' 를 사용하는 이유\n",
    "- cv의 값을 크게 할 수록 교차 검증의 횟수 또한 증가\n",
    "\n",
    "neg_mean_squared_error가 음수가 나온다. 그러나 MSE는 비용함수이므로 음수값을 붙여서 효용함수로 바꿔줘야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                                  scoring='neg_mean_squared_error', \n",
    "                                  n_jobs=-1) \n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d1fbd5",
   "metadata": {},
   "source": [
    "### Evaluate the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3382743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(0.5)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab51ea8",
   "metadata": {},
   "source": [
    "### Define the ensemble\n",
    "예제 참고 : https://hleecaster.com/ml-logistic-regression-example/  \n",
    "from sklearn.linear_model import LogisticRegression 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED=1\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNN(n_neighbors=27)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90dac6",
   "metadata": {},
   "source": [
    "### Evaluate individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    "  \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "  \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "  \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb1984",
   "metadata": {},
   "source": [
    "### Better performance with a Voting Classifier\n",
    "\n",
    "###### 코드 참고 : https://inuplace.tistory.com/569\n",
    "\n",
    "보팅분류기 모듈 = 다수결 분류     \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "- 보팅 분류기가 더 우수한 성능을 보임\n",
    "\n",
    "1) hard voting \n",
    "여러 모델을 생성하고 그 성과(결과) 비교   \n",
    "이때 classifier의 결과들을 집계해 가장 많은 표를 얻은 클래스를 최종 예측값으로 정하는 것       \n",
    "\n",
    "2) soft voting  \n",
    "앙상불에 사용되는 모든 분류기가 클래스의 확률을 예측할 수 있을 때 사용    \n",
    "각 분류기의 예측을 평균 내어 확률이 가장 높은 클래스로 예측 (= 가중치 투표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fa3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VotingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc \n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a57903e",
   "metadata": {},
   "source": [
    "# 03) Bagging and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b3e84",
   "metadata": {},
   "source": [
    "### Define the bagging classifier\n",
    "###### 코드 참고 : https://inuplace.tistory.com/571\n",
    "###### 배깅\n",
    "\n",
    "- 중복을 허용한 랜덤 샘플링으로 만든 훈련 세트(부트스트랩)를 사용하여 각 내부 모델을 다르게 학습시킴\n",
    "- 분류기가 predict_proba() 메소드를 지원하는 경우 확률값을 평균하여 예측을 수행한다. (소프트보팅 차용)\n",
    "- 없는 경우 가장 빈도가 높은 클래스 레이블이 예측 결과가 된다. (하드보팅 차용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d514d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80aeee3",
   "metadata": {},
   "source": [
    "### Evaluate Bagging performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072f6af",
   "metadata": {},
   "source": [
    "### Prepare the ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070bd901",
   "metadata": {},
   "source": [
    "### OOB Score vs Test Set Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ce7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9f2d7",
   "metadata": {},
   "source": [
    "### Train an RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2)\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4ed09",
   "metadata": {},
   "source": [
    "### Evaluate the RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b757bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = (MSE(y_test, y_pred))**(0.5)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7c1c3",
   "metadata": {},
   "source": [
    "### Visualizing features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a96398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_,\n",
    "                        index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e028149",
   "metadata": {},
   "source": [
    "# 04) Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ffef17",
   "metadata": {},
   "source": [
    "### Define the AdaBoost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f907e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f7c8b",
   "metadata": {},
   "source": [
    "### Train the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ada to the training set\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02bdcd",
   "metadata": {},
   "source": [
    "### Evaluate the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9be36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba905c3e",
   "metadata": {},
   "source": [
    "### Define the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4, \n",
    "            n_estimators=200,\n",
    "            random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5b72b",
   "metadata": {},
   "source": [
    "### Train the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786296f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit gb to the training set\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b6c9f",
   "metadata": {},
   "source": [
    "### Evaluate the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test**(1/2)\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d102d",
   "metadata": {},
   "source": [
    "### Regression with SGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, \n",
    "            subsample=0.9,\n",
    "            max_features=0.75,\n",
    "            n_estimators=200,\n",
    "            random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd87e98",
   "metadata": {},
   "source": [
    "### Train the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41cf60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b10dc9",
   "metadata": {},
   "source": [
    "### Evaluate the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1462b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7339c9f",
   "metadata": {},
   "source": [
    "# 05) Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a9956",
   "metadata": {},
   "source": [
    "### Set the tree's hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57fbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params_dt\n",
    "params_dt = {\n",
    "             'max_depth': [2, 3, 4],\n",
    "             'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcd7bb",
   "metadata": {},
   "source": [
    "### Search for the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84260e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b1aba",
   "metadata": {},
   "source": [
    "### Evaluate the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4959f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import roc_auc_score from sklearn.metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50e030",
   "metadata": {},
   "source": [
    "### Set the hyperparameter grid of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14400ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary 'params_rf'\n",
    "params_rf = {\n",
    "             'n_estimators': [100, 350, 500],\n",
    "             'max_features': ['log2', 'auto', 'sqrt'],\n",
    "             'min_samples_leaf': [2, 10, 30], \n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea1b1b",
   "metadata": {},
   "source": [
    "### Search for the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fd736",
   "metadata": {},
   "source": [
    "### Evaluate the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1846319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e541a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef25e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e184a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
