{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac795206",
   "metadata": {},
   "source": [
    "# 01) Classification with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67bc5c",
   "metadata": {},
   "source": [
    "### XGBoost: Fit/Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train,X_test,y_train,y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e19f7",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6756c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9be19c",
   "metadata": {},
   "source": [
    "### Measuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the DMatrix from X and y: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                    nfold=3, num_boost_round=5, \n",
    "                    metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03349e",
   "metadata": {},
   "source": [
    "### Measuring AUC\n",
    "\n",
    "train-auc-mean  train-auc-std  test-auc-mean  test-auc-std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                    nfold=3, num_boost_round=5, \n",
    "                    metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859f534",
   "metadata": {},
   "source": [
    "# 02) Regression with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32665b43",
   "metadata": {},
   "source": [
    "### Decision trees as base learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6ec49",
   "metadata": {},
   "source": [
    "### Linear base learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44c28c",
   "metadata": {},
   "source": [
    "### Evaluating model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4bf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final round boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ab7da",
   "metadata": {},
   "source": [
    "### Using regularization in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "    params[\"lambda\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa7011f",
   "metadata": {},
   "source": [
    "### Visualizing individual XGBoost trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree\n",
    "xgb.plot_tree(xg_reg, num_trees=0)\n",
    "plt.show()\n",
    "\n",
    "# Plot the fifth tree\n",
    "xgb.plot_tree(xg_reg, num_trees=4)\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree sideways\n",
    "xgb.plot_tree(xg_reg, num_trees=9, rankdir='LR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac16f0",
   "metadata": {},
   "source": [
    "### Visualizing feature importances: What features are most important in my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a60729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2791374f",
   "metadata": {},
   "source": [
    "# 03) Fine-tuning your XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b245873",
   "metadata": {},
   "source": [
    "### Tuning the number of boosting rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e799de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89982256",
   "metadata": {},
   "source": [
    "### Automated boosting round selection using early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66822ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=50, early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5be343",
   "metadata": {},
   "source": [
    "### Tuning eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b54993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta\n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n",
    "                        num_boost_round=10, early_stopping_rounds=5,\n",
    "                        metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebcced0",
   "metadata": {},
   "source": [
    "### Tuning max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff652c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc2da0",
   "metadata": {},
   "source": [
    "### Tuning colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba41d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5dbcb",
   "metadata": {},
   "source": [
    "### Grid search with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43bd015",
   "metadata": {},
   "source": [
    "### Random search with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc793013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,\n",
    "                                    n_iter=5, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \",randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51c070",
   "metadata": {},
   "source": [
    "# 04) Using XGBoost in pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d2e01",
   "metadata": {},
   "source": [
    "### Encoding categorical columns I: LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "print(df[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(df[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9989fd",
   "metadata": {},
   "source": [
    "### Encoding categorical columns II: OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "df_encoded = ohe.fit_transform(df)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print(df_encoded[:5, :])\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "print(df.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bc6a1",
   "metadata": {},
   "source": [
    "### Encoding categorical columns III: DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "df_dict = df.to_dict(\"records\")\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply dv on df: df_encoded\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Print the resulting first five rows\n",
    "print(df_encoded[:5,:])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b1f2f",
   "metadata": {},
   "source": [
    "### Preprocessing within a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae47167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X.to_dict(\"records\"), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f578dc7",
   "metadata": {},
   "source": [
    "### Cross-validating your XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d857cc",
   "metadata": {},
   "source": [
    "### Kidney disease case study I: Categorical Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature],Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1652569",
   "metadata": {},
   "source": [
    "### Kidney disease case study II: Feature Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b08646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda731d1",
   "metadata": {},
   "source": [
    "### Kidney disease case study III: Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "                    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e338f",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09844d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(.05, 1, .05),\n",
    "    'clf__max_depth': np.arange(3,10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,\n",
    "                                        param_distributions=gbm_param_grid,\n",
    "                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e552d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0633cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3f114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
